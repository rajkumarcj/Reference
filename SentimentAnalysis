import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.ml.classification.NaiveBayes;
import org.apache.spark.ml.classification.NaiveBayesModel;
import org.apache.spark.ml.feature.HashingTF;
import org.apache.spark.ml.feature.IDF;
import org.apache.spark.ml.feature.Tokenizer;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.feature.Word2Vec;
import org.apache.spark.ml.linalg.Vector;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructType;

public class SentimentAnalysis {

    public static void main(String[] args) {
        // Create a Spark session
        SparkSession spark = SparkSession.builder()
                .appName("SentimentAnalysis")
                .config("spark.master", "local")
                .getOrCreate();

        // Sample data for training
        JavaRDD<Row> trainingData = spark.read()
                .textFile("path/to/trainingData.txt")
                .javaRDD()
                .map(line -> RowFactory.create(0.0, line));

        // Define a schema for the training data
        StructType schema = new StructType()
                .add("label", DataTypes.DoubleType, false)
                .add("text", DataTypes.StringType, false);

        // Create a DataFrame from the training data
        Dataset<Row> trainingDataFrame = spark.createDataFrame(trainingData, schema);

        // Tokenize the text
        Tokenizer tokenizer = new Tokenizer()
                .setInputCol("text")
                .setOutputCol("words");
        Dataset<Row> wordsData = tokenizer.transform(trainingDataFrame);

        // Convert words into feature vectors
        HashingTF hashingTF = new HashingTF()
                .setInputCol("words")
                .setOutputCol("rawFeatures")
                .setNumFeatures(10000);
        Dataset<Row> featurizedData = hashingTF.transform(wordsData);

        // Add IDF (Inverse Document Frequency) to the features
        IDF idf = new IDF()
                .setInputCol("rawFeatures")
                .setOutputCol("features");
        idf.fit(featurizedData);

        // Create a Naive Bayes model
        NaiveBayes nb = new NaiveBayes()
                .setLabelCol("label")
                .setFeaturesCol("features")
                .setModelType("multinomial");  // Adjust according to your data

        // Train the model
        NaiveBayesModel model = nb.fit(trainingDataFrame);

        // Sample data for prediction
        JavaRDD<Row> testData = spark.read()
                .textFile("path/to/testData.txt")
                .javaRDD()
                .map(line -> RowFactory.create(line));

        // Define a schema for the test data
        StructType testSchema = new StructType()
                .add("text", DataTypes.StringType, false);

        // Create a DataFrame from the test data
        Dataset<Row> testDataFrame = spark.createDataFrame(testData, testSchema);

        // Tokenize and featurize the test data
        Dataset<Row> testWordsData = tokenizer.transform(testDataFrame);
        Dataset<Row> testFeaturizedData = hashingTF.transform(testWordsData);

        // Make predictions
        Dataset<Row> predictions = model.transform(testFeaturizedData);

        // Show the results
        predictions.select("text", "prediction").show();

        // Stop the Spark session
        spark.stop();
    }
}


<dependencies>
    <!-- Spark dependencies -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.12</artifactId>
        <version>3.1.2</version> <!-- Replace with the latest Spark version -->
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.12</artifactId>
        <version>3.1.2</version>
    </dependency>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-mllib_2.12</artifactId>
        <version>3.1.2</version>
    </dependency>

    <!-- Apache Commons for handling RowFactory -->
    <dependency>
        <groupId>org.apache.commons</groupId>
        <artifactId>commons-lang3</artifactId>
        <version>3.12.0</version> <!-- Replace with the latest version -->
    </dependency>

    <!-- Scala library for Spark -->
    <dependency>
        <groupId>org.scala-lang</groupId>
        <artifactId>scala-library</artifactId>
        <version>2.12.15</version> <!-- Replace with the latest Scala version compatible with Spark -->
    </dependency>
</dependencies>




